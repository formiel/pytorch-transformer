********************************************************
2 layers
********************************************************

******* LAYERDROP = 0.0
Start training
  200/ 2981 batches | lr 5.00 | ms/batch 24.04 | loss  8.05 | ppl  3123.52                                                                                                                                  
  400/ 2981 batches | lr 5.00 | ms/batch 23.41 | loss  6.78 | ppl   881.60
  600/ 2981 batches | lr 5.00 | ms/batch 24.50 | loss  6.38 | ppl   589.18
  800/ 2981 batches | lr 5.00 | ms/batch 23.34 | loss  6.23 | ppl   509.18
 1000/ 2981 batches | lr 5.00 | ms/batch 23.55 | loss  6.11 | ppl   451.52
 1200/ 2981 batches | lr 5.00 | ms/batch 23.45 | loss  6.08 | ppl   436.59
 1400/ 2981 batches | lr 5.00 | ms/batch 23.94 | loss  6.04 | ppl   418.72
 1600/ 2981 batches | lr 5.00 | ms/batch 23.82 | loss  6.04 | ppl   420.69
 1800/ 2981 batches | lr 5.00 | ms/batch 23.89 | loss  5.95 | ppl   383.29
 2000/ 2981 batches | lr 5.00 | ms/batch 24.03 | loss  5.96 | ppl   387.52
 2200/ 2981 batches | lr 5.00 | ms/batch 24.05 | loss  5.84 | ppl   345.49
 2400/ 2981 batches | lr 5.00 | ms/batch 24.19 | loss  5.90 | ppl   364.41
 2600/ 2981 batches | lr 5.00 | ms/batch 23.97 | loss  5.89 | ppl   362.49
 2800/ 2981 batches | lr 5.00 | ms/batch 24.83 | loss  5.79 | ppl   327.93
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 73.78s | valid loss  5.77 | valid ppl   322.00
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.75 | ms/batch 24.30 | loss  5.80 | ppl   331.64
  400/ 2981 batches | lr 4.75 | ms/batch 24.17 | loss  5.78 | ppl   322.44
  600/ 2981 batches | lr 4.75 | ms/batch 24.09 | loss  5.60 | ppl   271.03
  800/ 2981 batches | lr 4.75 | ms/batch 24.09 | loss  5.65 | ppl   283.17
 1000/ 2981 batches | lr 4.75 | ms/batch 24.02 | loss  5.59 | ppl   267.14
 1200/ 2981 batches | lr 4.75 | ms/batch 24.13 | loss  5.62 | ppl   275.07
 1400/ 2981 batches | lr 4.75 | ms/batch 23.87 | loss  5.63 | ppl   278.03
 1600/ 2981 batches | lr 4.75 | ms/batch 23.54 | loss  5.67 | ppl   288.67
 1800/ 2981 batches | lr 4.75 | ms/batch 23.98 | loss  5.58 | ppl   265.23
 2000/ 2981 batches | lr 4.75 | ms/batch 23.94 | loss  5.61 | ppl   274.28
 2200/ 2981 batches | lr 4.75 | ms/batch 23.06 | loss  5.51 | ppl   246.66
 2400/ 2981 batches | lr 4.75 | ms/batch 23.32 | loss  5.57 | ppl   262.88
 2600/ 2981 batches | lr 4.75 | ms/batch 23.87 | loss  5.58 | ppl   265.54
 2800/ 2981 batches | lr 4.75 | ms/batch 24.21 | loss  5.51 | ppl   247.32
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 73.46s | valid loss  5.55 | valid ppl   256.35
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.51 | ms/batch 24.03 | loss  5.54 | ppl   254.26
  400/ 2981 batches | lr 4.51 | ms/batch 22.87 | loss  5.55 | ppl   256.35
  600/ 2981 batches | lr 4.51 | ms/batch 23.24 | loss  5.36 | ppl   212.53
  800/ 2981 batches | lr 4.51 | ms/batch 24.01 | loss  5.42 | ppl   225.08
 1000/ 2981 batches | lr 4.51 | ms/batch 24.22 | loss  5.37 | ppl   214.48
 1200/ 2981 batches | lr 4.51 | ms/batch 24.16 | loss  5.41 | ppl   223.87
 1400/ 2981 batches | lr 4.51 | ms/batch 24.26 | loss  5.44 | ppl   229.38
 1600/ 2981 batches | lr 4.51 | ms/batch 24.23 | loss  5.48 | ppl   239.02
 1800/ 2981 batches | lr 4.51 | ms/batch 24.71 | loss  5.41 | ppl   223.04
 2000/ 2981 batches | lr 4.51 | ms/batch 23.22 | loss  5.43 | ppl   227.80
 2200/ 2981 batches | lr 4.51 | ms/batch 24.02 | loss  5.32 | ppl   203.41
 2400/ 2981 batches | lr 4.51 | ms/batch 25.04 | loss  5.40 | ppl   221.18
 2600/ 2981 batches | lr 4.51 | ms/batch 24.49 | loss  5.41 | ppl   223.26
 2800/ 2981 batches | lr 4.51 | ms/batch 23.90 | loss  5.34 | ppl   208.45
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 73.95s | valid loss  5.49 | valid ppl   241.64
-----------------------------------------------------------------------------------------


******* LAYERDROP = 0.2

Start training
  200/ 2981 batches | lr 5.00 | ms/batch 23.62 | loss  8.12 | ppl  3361.87
  400/ 2981 batches | lr 5.00 | ms/batch 23.20 | loss  6.96 | ppl  1049.05
  600/ 2981 batches | lr 5.00 | ms/batch 22.90 | loss  6.49 | ppl   661.45
  800/ 2981 batches | lr 5.00 | ms/batch 23.42 | loss  6.37 | ppl   585.92
 1000/ 2981 batches | lr 5.00 | ms/batch 23.32 | loss  6.24 | ppl   515.28
 1200/ 2981 batches | lr 5.00 | ms/batch 23.64 | loss  6.22 | ppl   500.64
 1400/ 2981 batches | lr 5.00 | ms/batch 23.20 | loss  6.14 | ppl   463.85
 1600/ 2981 batches | lr 5.00 | ms/batch 23.33 | loss  6.17 | ppl   478.65
 1800/ 2981 batches | lr 5.00 | ms/batch 23.89 | loss  6.02 | ppl   413.54
 2000/ 2981 batches | lr 5.00 | ms/batch 23.37 | loss  6.09 | ppl   442.73
 2200/ 2981 batches | lr 5.00 | ms/batch 23.68 | loss  5.94 | ppl   381.28
 2400/ 2981 batches | lr 5.00 | ms/batch 22.96 | loss  5.99 | ppl   400.89
 2600/ 2981 batches | lr 5.00 | ms/batch 23.29 | loss  5.97 | ppl   390.63
 2800/ 2981 batches | lr 5.00 | ms/batch 23.14 | loss  5.92 | ppl   372.45
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 72.09s | valid loss  5.88 | valid ppl   358.17
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.75 | ms/batch 22.88 | loss  5.93 | ppl   375.06
  400/ 2981 batches | lr 4.75 | ms/batch 23.61 | loss  5.81 | ppl   333.02
  600/ 2981 batches | lr 4.75 | ms/batch 23.17 | loss  5.74 | ppl   310.90
  800/ 2981 batches | lr 4.75 | ms/batch 23.97 | loss  5.75 | ppl   314.19
 1000/ 2981 batches | lr 4.75 | ms/batch 23.83 | loss  5.72 | ppl   303.87
 1200/ 2981 batches | lr 4.75 | ms/batch 22.94 | loss  5.73 | ppl   308.31
 1400/ 2981 batches | lr 4.75 | ms/batch 23.79 | loss  5.74 | ppl   310.84
 1600/ 2981 batches | lr 4.75 | ms/batch 23.74 | loss  5.78 | ppl   324.68
 1800/ 2981 batches | lr 4.75 | ms/batch 24.02 | loss  5.74 | ppl   310.77
 2000/ 2981 batches | lr 4.75 | ms/batch 23.60 | loss  5.70 | ppl   299.26
 2200/ 2981 batches | lr 4.75 | ms/batch 23.77 | loss  5.68 | ppl   293.85
 2400/ 2981 batches | lr 4.75 | ms/batch 23.90 | loss  5.70 | ppl   300.28
 2600/ 2981 batches | lr 4.75 | ms/batch 23.64 | loss  5.72 | ppl   304.78
 2800/ 2981 batches | lr 4.75 | ms/batch 23.15 | loss  5.69 | ppl   296.03
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 72.43s | valid loss  5.63 | valid ppl   278.10
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.51 | ms/batch 23.38 | loss  5.65 | ppl   284.53
  400/ 2981 batches | lr 4.51 | ms/batch 23.43 | loss  5.67 | ppl   289.08
  600/ 2981 batches | lr 4.51 | ms/batch 23.56 | loss  5.51 | ppl   247.27
  800/ 2981 batches | lr 4.51 | ms/batch 23.39 | loss  5.62 | ppl   276.27
 1000/ 2981 batches | lr 4.51 | ms/batch 23.39 | loss  5.53 | ppl   252.01
 1200/ 2981 batches | lr 4.51 | ms/batch 23.98 | loss  5.54 | ppl   255.92
 1400/ 2981 batches | lr 4.51 | ms/batch 24.07 | loss  5.51 | ppl   248.38
 1600/ 2981 batches | lr 4.51 | ms/batch 23.90 | loss  5.62 | ppl   276.81
 1800/ 2981 batches | lr 4.51 | ms/batch 23.96 | loss  5.56 | ppl   259.56
 2000/ 2981 batches | lr 4.51 | ms/batch 23.95 | loss  5.50 | ppl   245.67
 2200/ 2981 batches | lr 4.51 | ms/batch 23.40 | loss  5.50 | ppl   243.97
 2400/ 2981 batches | lr 4.51 | ms/batch 23.55 | loss  5.58 | ppl   264.22
 2600/ 2981 batches | lr 4.51 | ms/batch 23.69 | loss  5.56 | ppl   259.75
 2800/ 2981 batches | lr 4.51 | ms/batch 23.10 | loss  5.55 | ppl   257.91
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 72.91s | valid loss  5.57 | valid ppl   262.35
-----------------------------------------------------------------------------------------


******* LAYERDROP = 0.5

Start training
  200/ 2981 batches | lr 5.00 | ms/batch 21.96 | loss  8.22 | ppl  3711.89
  400/ 2981 batches | lr 5.00 | ms/batch 21.90 | loss  7.19 | ppl  1323.32
  600/ 2981 batches | lr 5.00 | ms/batch 22.10 | loss  6.76 | ppl   865.86
  800/ 2981 batches | lr 5.00 | ms/batch 22.04 | loss  6.58 | ppl   718.66
 1000/ 2981 batches | lr 5.00 | ms/batch 22.58 | loss  6.49 | ppl   657.28
 1200/ 2981 batches | lr 5.00 | ms/batch 22.85 | loss  6.41 | ppl   607.23
 1400/ 2981 batches | lr 5.00 | ms/batch 22.02 | loss  6.34 | ppl   564.64
 1600/ 2981 batches | lr 5.00 | ms/batch 22.09 | loss  6.35 | ppl   574.87
 1800/ 2981 batches | lr 5.00 | ms/batch 22.32 | loss  6.23 | ppl   509.78
 2000/ 2981 batches | lr 5.00 | ms/batch 22.51 | loss  6.27 | ppl   528.13
 2200/ 2981 batches | lr 5.00 | ms/batch 22.33 | loss  6.13 | ppl   460.73
 2400/ 2981 batches | lr 5.00 | ms/batch 21.66 | loss  6.14 | ppl   463.07
 2600/ 2981 batches | lr 5.00 | ms/batch 22.01 | loss  6.14 | ppl   463.62
 2800/ 2981 batches | lr 5.00 | ms/batch 22.73 | loss  6.04 | ppl   421.86
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 68.52s | valid loss  5.86 | valid ppl   349.20
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.75 | ms/batch 22.90 | loss  5.95 | ppl   384.78
  400/ 2981 batches | lr 4.75 | ms/batch 22.85 | loss  5.98 | ppl   393.68
  600/ 2981 batches | lr 4.75 | ms/batch 22.35 | loss  5.78 | ppl   322.65
  800/ 2981 batches | lr 4.75 | ms/batch 22.32 | loss  5.81 | ppl   332.51
 1000/ 2981 batches | lr 4.75 | ms/batch 22.39 | loss  5.74 | ppl   311.25
 1200/ 2981 batches | lr 4.75 | ms/batch 21.94 | loss  5.83 | ppl   339.15
 1400/ 2981 batches | lr 4.75 | ms/batch 22.27 | loss  5.80 | ppl   330.72
 1600/ 2981 batches | lr 4.75 | ms/batch 22.38 | loss  5.84 | ppl   344.17
 1800/ 2981 batches | lr 4.75 | ms/batch 21.77 | loss  5.79 | ppl   326.45
 2000/ 2981 batches | lr 4.75 | ms/batch 22.32 | loss  5.77 | ppl   319.11
 2200/ 2981 batches | lr 4.75 | ms/batch 22.37 | loss  5.68 | ppl   292.82
 2400/ 2981 batches | lr 4.75 | ms/batch 22.29 | loss  5.71 | ppl   303.17
 2600/ 2981 batches | lr 4.75 | ms/batch 22.50 | loss  5.79 | ppl   327.86
 2800/ 2981 batches | lr 4.75 | ms/batch 22.04 | loss  5.68 | ppl   294.10
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 69.04s | valid loss  5.74 | valid ppl   311.35
-----------------------------------------------------------------------------------------
  200/ 2981 batches | lr 4.51 | ms/batch 21.80 | loss  5.66 | ppl   288.05
  400/ 2981 batches | lr 4.51 | ms/batch 22.42 | loss  5.69 | ppl   294.49
  600/ 2981 batches | lr 4.51 | ms/batch 22.12 | loss  5.50 | ppl   245.54
  800/ 2981 batches | lr 4.51 | ms/batch 22.87 | loss  5.54 | ppl   255.12
 1000/ 2981 batches | lr 4.51 | ms/batch 22.36 | loss  5.52 | ppl   248.50
 1200/ 2981 batches | lr 4.51 | ms/batch 22.85 | loss  5.55 | ppl   258.02
 1400/ 2981 batches | lr 4.51 | ms/batch 22.10 | loss  5.57 | ppl   262.05
 1600/ 2981 batches | lr 4.51 | ms/batch 23.19 | loss  5.60 | ppl   269.38
 1800/ 2981 batches | lr 4.51 | ms/batch 22.57 | loss  5.53 | ppl   250.91
 2000/ 2981 batches | lr 4.51 | ms/batch 23.25 | loss  5.54 | ppl   254.97
 2200/ 2981 batches | lr 4.51 | ms/batch 22.70 | loss  5.45 | ppl   233.80
 2400/ 2981 batches | lr 4.51 | ms/batch 22.35 | loss  5.52 | ppl   249.51
 2600/ 2981 batches | lr 4.51 | ms/batch 22.86 | loss  5.53 | ppl   253.20
 2800/ 2981 batches | lr 4.51 | ms/batch 23.02 | loss  5.48 | ppl   239.48
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 69.65s | valid loss  5.60 | valid ppl   270.23
-----------------------------------------------------------------------------------------


********************************************************
4 layers
********************************************************

LayerDrop = 0
Start training
  200/ 2981 batches | lr 5.00 | ms/batch 30.99 | loss  8.22 | ppl  3698.39
  400/ 2981 batches | lr 5.00 | ms/batch 29.08 | loss  7.41 | ppl  1653.61
  600/ 2981 batches | lr 5.00 | ms/batch 29.95 | loss  7.14 | ppl  1265.42
  800/ 2981 batches | lr 5.00 | ms/batch 30.08 | loss  7.06 | ppl  1167.70
 1000/ 2981 batches | lr 5.00 | ms/batch 31.24 | loss  6.97 | ppl  1065.45
 1200/ 2981 batches | lr 5.00 | ms/batch 31.08 | loss  6.83 | ppl   922.88
 1400/ 2981 batches | lr 5.00 | ms/batch 29.47 | loss  6.68 | ppl   795.53
 1600/ 2981 batches | lr 5.00 | ms/batch 31.52 | loss  6.66 | ppl   777.03
 1800/ 2981 batches | lr 5.00 | ms/batch 29.46 | loss  6.60 | ppl   736.60
 2000/ 2981 batches | lr 5.00 | ms/batch 32.05 | loss  6.62 | ppl   750.95
 2200/ 2981 batches | lr 5.00 | ms/batch 32.06 | loss  6.57 | ppl   715.74
 2400/ 2981 batches | lr 5.00 | ms/batch 30.94 | loss  6.55 | ppl   699.12
 2600/ 2981 batches | lr 5.00 | ms/batch 32.52 | loss  6.57 | ppl   715.45
 2800/ 2981 batches | lr 5.00 | ms/batch 31.45 | loss  6.52 | ppl   680.82
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 94.95s | valid loss  6.32 | valid ppl   556.51
-----------------------------------------------------------------------------------------

LayerDrop = 0.5
Start training
  200/ 2981 batches | lr 5.00 | ms/batch 26.12 | loss  8.46 | ppl  4740.43
  400/ 2981 batches | lr 5.00 | ms/batch 27.11 | loss  7.31 | ppl  1496.68
  600/ 2981 batches | lr 5.00 | ms/batch 26.78 | loss  6.90 | ppl   987.48
  800/ 2981 batches | lr 5.00 | ms/batch 26.55 | loss  6.79 | ppl   885.42
 1000/ 2981 batches | lr 5.00 | ms/batch 27.24 | loss  6.62 | ppl   747.14
 1200/ 2981 batches | lr 5.00 | ms/batch 27.28 | loss  6.60 | ppl   737.83
 1400/ 2981 batches | lr 5.00 | ms/batch 27.17 | loss  6.56 | ppl   708.94
 1600/ 2981 batches | lr 5.00 | ms/batch 27.75 | loss  6.52 | ppl   675.24
 1800/ 2981 batches | lr 5.00 | ms/batch 27.86 | loss  6.46 | ppl   635.90
 2000/ 2981 batches | lr 5.00 | ms/batch 27.25 | loss  6.47 | ppl   644.59
 2200/ 2981 batches | lr 5.00 | ms/batch 27.64 | loss  6.44 | ppl   627.74
 2400/ 2981 batches | lr 5.00 | ms/batch 27.38 | loss  6.38 | ppl   587.54
 2600/ 2981 batches | lr 5.00 | ms/batch 27.29 | loss  6.47 | ppl   647.57
 2800/ 2981 batches | lr 5.00 | ms/batch 27.45 | loss  6.40 | ppl   600.59
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 84.51s | valid loss  6.15 | valid ppl   469.62
-----------------------------------------------------------------------------------------


2 GPUS

LayerDrop = 0.0
Start training
  200/ 1490 batches | lr 10.00 | ms/batch 111.86 | loss  9.30 | ppl 10951.91
  400/ 1490 batches | lr 10.00 | ms/batch 97.22 | loss  8.50 | ppl  4914.35
  600/ 1490 batches | lr 10.00 | ms/batch 98.79 | loss  8.03 | ppl  3078.59
  800/ 1490 batches | lr 10.00 | ms/batch 92.44 | loss  7.53 | ppl  1864.10
 1000/ 1490 batches | lr 10.00 | ms/batch 94.62 | loss  7.35 | ppl  1560.13
 1200/ 1490 batches | lr 10.00 | ms/batch 95.37 | loss  7.30 | ppl  1484.62
 1400/ 1490 batches | lr 10.00 | ms/batch 99.16 | loss  7.22 | ppl  1372.88
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 153.57s | valid loss  6.77 | valid ppl   874.91
-----------------------------------------------------------------------------------------

LayerDrop = 0.5
Start training
  200/ 1490 batches | lr 10.00 | ms/batch 104.92 | loss  9.30 | ppl 10898.58
  400/ 1490 batches | lr 10.00 | ms/batch 87.34 | loss  8.10 | ppl  3305.46
  600/ 1490 batches | lr 10.00 | ms/batch 87.89 | loss  7.79 | ppl  2418.99
  800/ 1490 batches | lr 10.00 | ms/batch 85.88 | loss  7.60 | ppl  2004.69
 1000/ 1490 batches | lr 10.00 | ms/batch 86.72 | loss  7.52 | ppl  1844.90
 1200/ 1490 batches | lr 10.00 | ms/batch 86.30 | loss  7.63 | ppl  2050.25
 1400/ 1490 batches | lr 10.00 | ms/batch 88.11 | loss  7.65 | ppl  2106.01
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 140.19s | valid loss  6.97 | valid ppl  1065.24
-----------------------------------------------------------------------------------------


